{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognition\n",
    "\n",
    "The MNIST handwritten digit classification problem is a standard dataset used in computer vision and deep learning.\n",
    "\n",
    "Although the dataset is effectively solved, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch. This includes how to develop a robust test harness for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.\n",
    "\n",
    "The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset.\n",
    "\n",
    "It is a dataset of 60,000 small square 28Ã—28 pixel grayscale images of handwritten single digits between 0 and 9.\n",
    "\n",
    "The task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively.\n",
    "\n",
    "![Mnist examples](./assets/mnist%20examples.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the dataset\n",
    "(X1, Y1), (X2, Y2) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process the dataset\n",
    "m_train = X1.shape[0]\n",
    "m_test = X2.shape[0]\n",
    "X_train = (X1.reshape(X1.shape[0],-1).T)/255\n",
    "Y_train_temp = Y1.reshape(Y1.shape[0],)\n",
    "Y_train = np.zeros((Y_train_temp.size,10))\n",
    "Y_train[np.arange(Y_train_temp.size),Y_train_temp] = 1\n",
    "Y_train = Y_train.T\n",
    "X_test = (X2.reshape(X2.shape[0],-1).T)/255\n",
    "Y_test_temp = Y2.reshape(Y2.shape[0],)\n",
    "Y_test = np.zeros((Y_test_temp.size,10))\n",
    "Y_test[np.arange(Y_test_temp.size),Y_test_temp] = 1\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining activation functions\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def leakyrelu(z):\n",
    "    s = np.where(z>0 , z , z*0.01)\n",
    "    activation_cache = (z)\n",
    "    return s, activation_cache\n",
    "\n",
    "def softmax(z):\n",
    "    s = np.exp(z)/np.sum(np.exp(z), axis = 0, keepdims = True)\n",
    "    activation_cache = (z)\n",
    "    return s, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X,Y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- Input image\n",
    "    Y -- Label of input image\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = np.random.randn(100,X.shape[0])*0.01\n",
    "    b1 = np.zeros((100,1), dtype = float)\n",
    "    W2 = np.random.randn(50,W1.shape[0])*0.01\n",
    "    b2 = np.zeros((50,1), dtype = float)\n",
    "    W3 = np.random.randn(25,W2.shape[0])*0.01\n",
    "    b3 = np.zeros((25,1), dtype = float)\n",
    "    W4 = np.random.randn(10,W3.shape[0])*0.01\n",
    "    b4 = np.zeros((10,1), dtype = float)\n",
    "\n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1,\"W2\" : W2, \"b2\" : b2,\"W3\" : W3, \"b3\" : b3,\"W4\" : W4, \"b4\" : b4,}\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "     \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "     \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"leakyrelu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = leakyrelu(Z)\n",
    "    \n",
    "    if activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for l in range (1,L):\n",
    "        A_prev = A\n",
    "\n",
    "        A,cache = linear_activation_forward(A_prev,parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"leakyrelu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache =  linear_activation_forward(A,parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"softmax\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "     \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = - np.sum(Y*np.log(AL))/m\n",
    "    np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ , cache):\n",
    "     \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(AL, Y):\n",
    "    dZ = AL- Y\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    Z_temp = np.where(Z>0, 1, 0.01)\n",
    "    dZ = dA * Z_temp\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(Y, AL, dA, cache, activation):\n",
    "     \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"leakyrelu\":\n",
    "        dZ = leakyrelu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(AL, Y)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = -Y/AL\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(Y,AL,dAL,current_cache, \"softmax\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(Y,AL,dA_prev_temp, current_cache, \"leakyrelu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l+1)] = dW_temp\n",
    "        grads[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params,grads,learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = params.copy()\n",
    "\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X,Y, mini_batch_size = 64):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    inc = mini_batch_size\n",
    "    num_complete_minibatches = m // mini_batch_size\n",
    "   \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k*inc:(k+1)*inc]\n",
    "        mini_batch_Y = shuffled_Y[:,k*inc:(k+1)*inc]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(Y_hat):\n",
    "    return np.argmax(Y_hat,0)\n",
    "\n",
    "def get_accuracy(predictions,Y):\n",
    "    predictions = predictions.reshape(1,predictions.shape[0])\n",
    "    #print(predictions.shape)\n",
    "    ans = 0\n",
    "    for i in range(Y.shape[1]) :\n",
    "        predict = predictions[0,i]\n",
    "        if Y[predict,i]==1 :\n",
    "            ans+=1\n",
    "    print(ans)\n",
    "    return str((ans/Y.shape[1])*100) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(X, Y, learning_rate = 0.0075, num_iterations = 3000, print_cost = False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    costs -- An array of the costs of every iteration\n",
    "    Y_predict -- A one hot encoded numpy array for calculating accuracy.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    parameters = initialize_parameters(X,Y)\n",
    "    cost = 2.5\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_forward(X,parameters)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        Y_predict = np.zeros(AL.shape)\n",
    "        Y_predict[np.argmax(AL, axis = 0), np.arange(AL.shape[1])] = 1\n",
    "        \n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            #print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_predict - Y)) * 100))\n",
    "            print(\"accuracy : \" , get_accuracy(get_predictions(AL),Y))        \n",
    "\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters,costs, Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.302585642974819\n",
      "5156\n",
      "accuracy :  8.593333333333334%\n",
      "Cost after iteration 100: 2.3011558163568706\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 200: 2.3011516013839075\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 300: 2.301141747687725\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 400: 2.3011033446568465\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 500: 2.300350834684254\n",
      "6742\n",
      "accuracy :  11.236666666666666%\n",
      "Cost after iteration 600: 2.070144149579087\n",
      "13211\n",
      "accuracy :  22.018333333333334%\n",
      "Cost after iteration 700: 2.251270035035066\n",
      "7157\n",
      "accuracy :  11.928333333333335%\n",
      "Cost after iteration 800: 1.2424526680424275\n",
      "28185\n",
      "accuracy :  46.975%\n",
      "Cost after iteration 900: 1.5757071058293326\n",
      "20467\n",
      "accuracy :  34.111666666666665%\n",
      "Cost after iteration 1000: 0.5046285050816177\n",
      "49035\n",
      "accuracy :  81.72500000000001%\n",
      "Cost after iteration 1100: 3.2116518083707457\n",
      "28107\n",
      "accuracy :  46.845%\n",
      "Cost after iteration 1200: 0.2576492593134629\n",
      "55773\n",
      "accuracy :  92.955%\n",
      "Cost after iteration 1300: 0.2629709061139808\n",
      "55279\n",
      "accuracy :  92.13166666666666%\n",
      "Cost after iteration 1400: 0.32535350798218726\n",
      "54214\n",
      "accuracy :  90.35666666666667%\n",
      "Cost after iteration 1500: 0.20866761675773926\n",
      "56470\n",
      "accuracy :  94.11666666666667%\n",
      "Cost after iteration 1600: 0.17021631296982215\n",
      "57100\n",
      "accuracy :  95.16666666666667%\n",
      "Cost after iteration 1700: 0.2145320534819292\n",
      "56276\n",
      "accuracy :  93.79333333333332%\n",
      "Cost after iteration 1800: 0.18372653353737403\n",
      "56804\n",
      "accuracy :  94.67333333333333%\n",
      "Cost after iteration 1900: 0.5388748750176362\n",
      "51699\n",
      "accuracy :  86.165%\n",
      "Cost after iteration 2000: 0.14676202100185817\n",
      "57457\n",
      "accuracy :  95.76166666666667%\n",
      "Cost after iteration 2100: 0.18688872148095378\n",
      "56595\n",
      "accuracy :  94.325%\n",
      "Cost after iteration 2200: 0.13332271106490712\n",
      "57692\n",
      "accuracy :  96.15333333333334%\n",
      "Cost after iteration 2300: 0.12038528361068891\n",
      "57895\n",
      "accuracy :  96.49166666666666%\n",
      "Cost after iteration 2400: 0.11745493663388569\n",
      "57939\n",
      "accuracy :  96.565%\n",
      "Cost after iteration 2499: 0.10477345682525653\n",
      "58192\n",
      "accuracy :  96.98666666666666%\n"
     ]
    }
   ],
   "source": [
    "parameters, costs, Y_predict = model(X_train,Y_train, 0.8, 2500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(parameters, X, Y):\n",
    "    AL, caches = L_forward(X,parameters)\n",
    "    print(\"accuracy : \" , get_accuracy(get_predictions(AL),Y))\n",
    "\n",
    "    return AL, Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9551\n",
      "accuracy :  95.50999999999999%\n"
     ]
    }
   ],
   "source": [
    "AL, Y_predict = accuracy_test(parameters, X_test, Y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c72393390aee9815d7b792bc598013fc10f52cf49f264bfd5dd08c1c22e3d87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
